{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IVA Arabic Visual Question Answering Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model based on the paper \"Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge\": \n",
    "https://openaccess.thecvf.com/content_cvpr_2018/papers/Teney_Tips_and_Tricks_CVPR_2018_paper.pdf \n",
    "\n",
    "Trained using VQA translated annotations using Google Translate and COCO dataset: https://visualqa.org/vqa_v1_download.html\n",
    "\n",
    "And using word embeddings of AraVec: https://github.com/bakrianoo/aravec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import csv\n",
    "import sys\n",
    "import base64\n",
    "import nltk\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as KB\n",
    "import pandas as pd\n",
    "from keras.preprocessing import image\n",
    "import gensim\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A helper function to download our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(folder_name, origin):\n",
    "    if not os.path.exists(os.path.abspath('.') + '/' + folder_name):\n",
    "        file_zip = tf.keras.utils.get_file(folder_name + '.zip',\n",
    "                                           cache_subdir=os.path.abspath('.'),\n",
    "                                           origin = origin,\n",
    "                                           extract = True)\n",
    "        PATH = os.path.dirname(file_zip)+'/' + folder_name + '/'\n",
    "    else:\n",
    "        PATH = os.path.abspath('.')+'/' + folder_name + '/'\n",
    "    os.remove(folder_name + '.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading word embeddings from AraVec resource\n",
    "download('arabic', 'https://bakrianoo.s3-us-west-2.amazonaws.com/aravec/full_grams_cbow_300_twitter.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download image files. Make sure to download them once only and to have 20 GB of free space!\n",
    "download('train2014', 'http://images.cocodataset.org/zips/train2014.zip')\n",
    "download('val2014', 'http://images.cocodataset.org/zips/val2014.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to clean strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used by AraVec to convert ى to ي and all types of أ آ to ا and similar stuff \n",
    "# to avoid errors resulting from misspelling\n",
    "\n",
    "def clean_str(text):\n",
    "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
    "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
    "    \n",
    "    #remove tashkeel\n",
    "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    text = re.sub(p_tashkeel,\"\", text)\n",
    "    \n",
    "    #remove longation\n",
    "    p_longation = re.compile(r'(.)\\1+')\n",
    "    subst = r\"\\1\\1\"\n",
    "    text = re.sub(p_longation, subst, text)\n",
    "    \n",
    "    text = text.replace('وو', 'و')\n",
    "    text = text.replace('يي', 'ي')\n",
    "    text = text.replace('اا', 'ا')\n",
    "    \n",
    "    for i in range(0, len(search)):\n",
    "        text = text.replace(search[i], replace[i])\n",
    "    \n",
    "    #trim    \n",
    "    text = text.strip()\n",
    "   \n",
    "\n",
    "    text = str(text)\n",
    "    text = re.sub(r'\\bال(\\w\\w+)', r'\\1', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our embedding vector\n",
    "word2vec = gensim.models.Word2Vec.load('full_grams_cbow_300_twitter.mdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lists will hold all the data both training and validation\n",
    "all_questions = []\n",
    "all_answers = []\n",
    "all_images = []\n",
    "all_img_names = [] # Full path image name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_excel('data/train.xlsx', engine='openpyxl')\n",
    "\n",
    "PATH = 'train2014/'\n",
    "train_questions = train_data['questions']\n",
    "train_answers = train_data['answers']\n",
    "train_images = train_data['images']\n",
    "\n",
    "for names in train_images:\n",
    "    image_path = PATH + names + '.jpg'\n",
    "    all_img_names.append(image_path)\n",
    "    \n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending training data to the general list\n",
    "for i in range(len(train_questions)):\n",
    "    all_questions.append(train_questions[i])\n",
    "    all_answers.append(train_answers[i])\n",
    "    all_images.append(train_images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample:\n",
    "index = 100\n",
    "plt.imshow(image.load_img(all_img_names[index]))\n",
    "all_questions[index], ' ', all_answers[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Of course one downside of the data is that it's open-ended and not based on simple visual descriptions, unfortunately half of the data \n",
    "###### is like this and the model could do a lot better to help visually-impaired people if it's trained on simpler samples, DL is not that smart yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we read validation data the same way and append it to the general list as well\n",
    "val_data = pd.read_excel('data/val.xlsx', engine='openpyxl')\n",
    "\n",
    "PATH = 'val2014/'\n",
    "val_questions = val_data['questions']\n",
    "val_answers = val_data['answers']\n",
    "val_images = val_data['images']\n",
    "\n",
    "for names in val_images:\n",
    "    image_path = PATH + names + '.jpg'\n",
    "    all_img_names.append(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_questions)):\n",
    "    all_questions.append(val_questions[i])\n",
    "    all_answers.append(val_answers[i])\n",
    "    all_images.append(val_images[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the strings in all questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(all_answers)):\n",
    "    all_questions[i] = clean_str(all_questions[i])\n",
    "    try:\n",
    "        all_answers[i] = clean_str(all_answers[i])\n",
    "    except TypeError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An optional step in case you want to save the data to load it and save the processing time later\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['questions'] = all_questions\n",
    "df['answers'] = all_answers\n",
    "df['images'] = all_images\n",
    "df.to_excel('data/all_cleaned.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model doesn't do well with counting. Hence, this step is aimed at replacing all numbers greater than three with the word كثير"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for i in range(0, len(all_answers)):\n",
    "    try:\n",
    "        if int(all_answers[i]) > 3:\n",
    "            all_answers[i] = 'كثير'\n",
    "    except ValueError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the top 40000 words from the vocabulary\n",
    "top_k = 40000  \n",
    "#we make one hot encoding (vector with a length = dictionary length) for the words from the dictionary \n",
    "ques_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='؟!?\"#$%&()*+.,-/:;=@[\\]^_`{|}~ ')\n",
    "ques_tokenizer.fit_on_texts(train_questions)\n",
    "\n",
    "#word_index is the dictionary or vocaulary of words \n",
    "ques_word_index = ques_tokenizer.word_index\n",
    "ques_index_word = ques_tokenizer.index_word\n",
    "print('Found %s unique tokens.' % len(ques_word_index))\n",
    "\n",
    "ques_tokenizer.word_index['<pad>'] = 0\n",
    "ques_tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "# Create the tokenized vectors\n",
    "question_seqs = ques_tokenizer.texts_to_sequences(train_questions)\n",
    "\n",
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "question_vector = tf.keras.preprocessing.sequence.pad_sequences(question_seqs, padding='post', truncating='post', maxlen=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample:\n",
    "ind = 1002\n",
    "train_data['questions'][ind], '  ', question_vector[ind] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save the tokenizer to use it later in testing or when creating an API\n",
    "tokenizer_json = ques_tokenizer.to_json()\n",
    "with open('data/vqa_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "embedding_matrix = np.zeros((len(ques_word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in tqdm(ques_word_index.items()):\n",
    "    if word in word2vec:\n",
    "        embedding_vector = word2vec[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    # We ignore words not in the embedding vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a dictionary holding the frequency of each answer, and then sorting the dictionary to take only the most common 1000 answers\n",
    "This approach has proven to provide the best result and about 30,000 samples from 440,000 samples weren’t\n",
    "included if we only take questions with answers residing in these 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_freq = {}\n",
    "for answer in train_answers:\n",
    "    if answer not in ans_freq:\n",
    "        ans_freq[answer] = 1\n",
    "    else:\n",
    "        ans_freq[answer] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_freq = {k: v for k, v in sorted(ans_freq.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the answers manually, topAnsWordIndex holds the mapping from an answer to its encoded value\n",
    "# topAnsIndexWord maps the index number to its encoded answer\n",
    "\n",
    "topAnsWordIndex = {}\n",
    "topAnsIndexWord = []\n",
    "\n",
    "cnt = 0\n",
    "for ans in ans_freq.keys():\n",
    "    topAnsWordIndex[ans] = cnt\n",
    "    topAnsIndexWord.append(ans)\n",
    "    \n",
    "    cnt += 1\n",
    "    if cnt==1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save this tokens list to use it later when testing the model or when creating an API\n",
    "pickle.dump(topAnsIndexWord, open('data/topAnsIndexWord.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the output layer of the model holding the index token of the specific answer\n",
    "# for example if the answer of the first question is \"نعم\" and this answer has a tokenization resulting from\n",
    "# the manual approach above of \"2\" then ans_encoded will have a value of 2 corresponding to that question\n",
    "\n",
    "ans_encoded = []\n",
    "for answer in train_answers:\n",
    "    if answer in topAnsWordIndex:\n",
    "        ans_encoded.append(topAnsWordIndex[answer])\n",
    "    else:\n",
    "        ans_encoded.append(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various image models have been tried, Xception had the better outcome by a small margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.xception.Xception(include_top=False, weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.xception.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique images feature matrix, make sure to have at least 70 GB of free space!\n",
    "\n",
    "from tqdm import tqdm\n",
    "encode = sorted(set(all_img_names))\n",
    "# Feel free to change batch_size according to your system configuration\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode)\n",
    "\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)\n",
    "\n",
    "for img, path in tqdm(image_dataset):\n",
    "    batch_features = image_features_extract_model(img)\n",
    "    batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "  \n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        np.save('features/'+path_of_feature[10:-4], bf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xception sample dimensions\n",
    "sample = np.load('features/COCO_train2014_000000000009.npy')\n",
    "print(type(sample))\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the layout of the model used, a small modifications to the model used in the referenced paper:\n",
    "\n",
    "![title](data/model_layout.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 15        # Largest squence used for questions word count\n",
    "embedding_dim = 300    # the dimension of our embedding matrix\n",
    "hidden = 1024          # Number of hidden units\n",
    "K = 100                # The K that is the second dimention of the image feature, from the sample above its (1, 100, 2048)\n",
    "v_dim = 2048\n",
    "OutputDim = len(topAnsWordIndex)  # Output layer dimentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data generator class to use batch training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=128, dim1=(K, v_dim), dim2=(seq_length,), dim3=(OutputDim,),\n",
    "                 n_classes=OutputDim, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim1 = dim1\n",
    "        self.dim2 = dim2\n",
    "        self.dim3 = dim3\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X1, X2, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return [X1, X2], y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' \n",
    "        # Initialization\n",
    "        X1 = np.empty((self.batch_size, *self.dim1))\n",
    "        X2 = np.empty((self.batch_size, *self.dim2))\n",
    "        y = np.empty((self.batch_size, *self.dim3), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X1[i, :, :] = np.load('features/' + images[int(ID)]+ '.npy')\n",
    "            X2[i,] = question_vector[int(ID)]\n",
    "            # Store class\n",
    "            temp = ans_encoded[int(ID)]\n",
    "            # Our output was encoded answers as one-number values, we convert it into a one-hot vector\n",
    "            y[i,] = to_categorical(temp, OutputDim)\n",
    "\n",
    "        return X1, X2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want samples that has answers in the top 1000, so we make a list of only these samples to use in the training\n",
    "ALL = len(all_images)\n",
    "lst = [str(i) for i in range(ALL) if ans_encoded[i] != -1]\n",
    "ALL = len(lst)\n",
    "ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating batches for training and validation data, of course this division has nothing to do with the train and val data \n",
    "# we read, we basically stacked them all together and then are using a smaller subset as validation\n",
    "\n",
    "training = DataGenerator(lst[:ALL-30000])\n",
    "validation = DataGenerator(lst[ALL-30000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model architecture implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = tf.keras.layers.Input(shape=(K, v_dim), name='image_input')\n",
    "\n",
    "question_input = tf.keras.layers.Input(shape=(seq_length,), dtype='int32', name='question_input')\n",
    "embedding = tf.keras.layers.Embedding(embedding_matrix.shape[0], embedding_dim, weights=[embedding_matrix],\n",
    "                                     input_length=seq_length, trainable=True, name='embedding')(question_input)\n",
    "\n",
    "rnn = tf.keras.layers.LSTM(units=hidden, name='rnn')(embedding)\n",
    "concat = tf.keras.layers.concatenate([image_input, tf.keras.backend.repeat(rnn, K)], name='concat')\n",
    "fc1 = tf.keras.layers.Dense(hidden, activation='relu', name='fc1')(concat)\n",
    "fc2 = tf.keras.layers.Dense(1, activation='softmax', name='fc2')(fc1)\n",
    "\n",
    "add = tf.keras.layers.Dot(axes=1, name='add')([fc2, image_input])\n",
    "add = tf.reshape(add, (-1, add.shape[2]))\n",
    "drop1 = tf.keras.layers.Dropout(0.3, name='drop1')(add)\n",
    "fc3 = tf.keras.layers.Dense(hidden, activation='relu', name='fc3')(drop1)\n",
    "fc4 = tf.keras.layers.Dense(hidden, activation='relu', name='fc4')(rnn)\n",
    "\n",
    "mul = tf.keras.layers.Multiply()([fc3, fc4])\n",
    "fc5 = tf.keras.layers.Dense(hidden+hidden, activation='relu', name='fc5')(mul)\n",
    "drop = tf.keras.layers.Dropout(0.5, name='dropout')(fc5)\n",
    "fc6 = tf.keras.layers.Dense(OutputDim, activation='softmax', name='fc6')(drop)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[image_input, question_input], outputs=fc6)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adamax(), loss=loss_object, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** It is recommended to use cloud resources for training as it will take a long time and can cause damage to even a good laptop, be careful. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(training, validation_data=validation, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well, we have to save it!\n",
    "model.save('data/themodel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test(img_path, question):\n",
    "    plt.imshow(image.load_img(img_path))\n",
    "    img = image.load_img(img_path, target_size=(299, 299))\n",
    "\n",
    "    # Getting the features of the matrix using the same image model\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = tf.keras.applications.xception.preprocess_input(x)\n",
    "\n",
    "    features = image_model.predict(x)\n",
    "    features.shape\n",
    "    X1 = features.reshape((1, 10*10, -1))\n",
    "\n",
    "    # Tokenizing the question\n",
    "    ques = clean_str(question)\n",
    "    X2 = ques_tokenizer.texts_to_sequences([ques])\n",
    "    X2 = tf.keras.preprocessing.sequence.pad_sequences(X2, padding='post',truncating='post',maxlen=15)\n",
    "\n",
    "    pred = model.predict([X1, X2])\n",
    "    print('Question: ', question)\n",
    "    \n",
    "    # Sorting the predictions based on higher probabilities and printing the top 3\n",
    "    pred2 = pred[0].argsort()[-5:][::-1]\n",
    "    print('ِProbability: \\t\\t Answer:')\n",
    "    cnt = 0\n",
    "    for i in pred2:\n",
    "        if pred[0][i] > 0.01:\n",
    "            print(pred[0][i], '\\t\\t', topAnsIndexWord[i])\n",
    "        cnt += 1\n",
    "        if cnt == 3:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test('data/test_image.jpg', 'ماذا في الخلفية؟')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hope you found this useful and hope you can improve on it in the future!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
